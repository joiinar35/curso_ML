{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkML0104ENSkillsNetwork3497-2023-01-01\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# **Machine Learning in Python**\n\nEstimated time needed: **30** minutes\n\nAfter completing this lab you will be able to:\n\n - Implement common supervised learning algorithms for regression and classification tasks\n - Implement dimensionality reduction\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## __Table of Contents__\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<ol>\n    <li>\n        <a href=\"#Setup\">Setup</a>\n        <ol>\n            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n        </ol>\n    </li>\n    <li><a href=\"#Training and Testing Data Splits\">Training and Testing Data Splits</a></li>\n    <li>\n        <a href=\"#Supervised-Learning\">Supervised Learning</a>\n        <ol>\n            <li><a href=\"#1.Regression-Models\">1. Regression Models</a></li>\n            <li><a href=\"#2.Classification-Models\">2. Classification Models</a></li>\n        </ol>\n    </li>\n    <li>\n        <a href=\"#Unsupervised-Learning\">Unsupervised Learning</a>\n        <ol>\n            <li><a href=\"#Dimensionality-Reduction-Using-Principal-Component-Analysis-(PCA)\">Dimensionality Reduction Using Principal Component Analysis (PCA)</a></li>\n        </ol>\n    </li>\n    <li>\n        <a href=\"#Exercises\">Exercises</a>\n        <ol>\n            <li><a href=\"#Exercise-1---Build-a-linear-regression-with-the-dataset-below-and-evaluate-the-model\">Exercise 1 - Build a linear regression with the dataset below and evaluate the model</a></li>\n            <li><a href=\"#Exercise-2---Build-a-logistic-regression-with-the-dataset-below-and-evaluate-the-model\">Build a logistic regression with the dataset below and evaluate the model</a></li>\n            <li><a href=\"#Exercise-3---Perform-PCA-on-the-data-below-to-plot-the-corresponding-Cumulative-Explained-Variance\">Exercise 3 - Perform PCA on the data below to plot the corresponding Cumulative Explained Variance</a></li>\n        </ol>\n    </li>\n    \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Setup\n\nFor this lab, we will be using the following libraries:\n\n*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n*   [`plotly`](https://plotly.com/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkML0104ENSkillsNetwork3497-2023-01-01) for extracting the data.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Importing Required Libraries\n\n_We recommend you import all required libraries in one place (here):_\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import (\n    confusion_matrix\n    \n)\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "# Training and Testing Data Splits\n<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-ML0104EN-SkillsNetwork/Screenshot%202023-06-17%20at%203.35.18%20AM.png\">\nIn this example, we generate a dataset with 100 samples and 5 features using numpy.random.rand. We also generate corresponding target labels (y) using numpy.random.randint. We then use train_test_split from sklearn.model_selection to split the data into training and testing sets, with a test size of 20% (test_size=0.2). Finally, we print the shapes of the training and testing sets to verify the sizes of the splits.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Create a dataset with 100 samples and 5 features\nnum_samples = 100\nnum_features = 5\nX = np.random.rand(num_samples, num_features)\ny = np.random.randint(0, 2, size=num_samples)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the training and testing sets\nprint(\"Training set shape:\", X_train.shape, y_train.shape)\nprint(\"Testing set shape:\", X_test.shape, y_test.shape)",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Supervised Learning\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Classification Models\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " Here's an explanation of the metrics used for the classification model we will build:\n #### 1. Confusion Matrix:\n The confusion matrix provides a summary of the model's performance by showing the counts of true positive, true negative, false positive, and false negative predictions.\n ||Actual Positive|Actual Negative|\n|-|-|-|\n**Predicted Positive** | True Positive   | False Negative  |\n**Predicted Negative** | False Positive  | True Negative   |\n\n$$ \\begin{bmatrix}\nTP & FN \\\\\nFP & TN \n\\end{bmatrix}$$\n\n#### 2. Accuracy:\nAccuracy measures the proportion of correct predictions out of the total predictions. It is calculated by dividing the sum of true positives and true negatives by the total number of samples.\n\n`Accuracy = (True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False \nNegatives)`\n$$Accurcy = \\frac{TP}{TP + FP + FP + FN}$$\n\n#### 3. Precision:\nPrecision quantifies the model's ability to correctly predict positive samples. It is the ratio of true positives to the sum of true positives and false positives. Precision reflects the model's accuracy when it predicts positive.\n\n`Precision = True Positives / (True Positives + False Positives)`\n$$Precision = \\frac{TP}{TP + FP}$$\n\n#### 4. Recall (Sensitivity or True Positive Rate):\nRecall, also known as sensitivity or true positive rate, measures the model's ability to correctly identify positive samples. It is the ratio of true positives to the sum of true positives and false negatives. Recall reflects the model's ability to capture all positive samples.\n\n`Recall = True Positives / (True Positives + False Negatives)`\n\n$$Recall = \\frac{TP}{TP + FN}$$\n\n#### 5. Specificity (True Negative Rate):\nSpecificity, also known as true negative rate, measures the model's ability to correctly identify negative samples. It is the ratio of true negatives to the sum of true negatives and false positives. Specificity reflects the model's accuracy when it predicts negative.\n\n`Specificity = True Negatives / (True Negatives + False Positives)`\n\n$$Specificity = \\frac{TN}{TN + TP}$$\n\n#### 6. F1 Score:\nThe F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall. The F1 score is useful when there is an uneven class distribution or when both precision and recall are equally important.\n\n`F1 Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate random data\nX = np.random.rand(200, 4)  # Input features\ny = np.random.randint(0, 2, size=200)  # Binary target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create logistic regression model\nmodel = LogisticRegression()\n\n# Fit the model on the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# True Positives\ntp = cm[1, 1]\n# False Positives\nfp = cm[0, 1]\n# False Negatives\nfn = cm[1, 0]\n# True Negatives\ntn = cm[0, 0]\n\n# Accuracy\naccuracy = (tp + tn) / (tp + tn + fp + fn)\n# Precision\nprecision = tp / (tp + fp)\n# Recall\nrecall = tp / (tp + fn)\n# Specificity\nspecificity = tn / (tn + fp)\n# F1 Score\nf1 = 2 * (precision * recall) / (precision + recall)\n\n\n# Print the evaluation metrics\nprint(\"Confusion Matrix:\", cm)\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint(\"Specificity:\", specificity)\nprint('F1 Score:', f1)",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Regression Models\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Here's an explanation of the metrics used for the regression model we will build:\n\n#### 1. Mean Squared Error (MSE):\nMSE measures the average squared difference between the predicted values and the actual values. It is calculated as follows:\n\n+ `MSE = (1/n) * Σ(y_actual - y_predicted)^2`\n\n$$MSE=\\frac{1}{n}\\sum_{i}^{n}(y_{i} - \\hat{y}_{i})^{2}$$ \n\nwhere:\n\n- `MSE` represents the Mean Squared Error.\n- `n` is the total number of data points.\n+- `Σ` denotes the summation symbol.\n- $y_{i}$ represents the actual values of the target variable.\n- $\\hat{y}_{i}$ represents the predicted values from the linear regression model.\nIn the code, `mean_squared_error(y_test, y_pred)` calculates the MSE for the dataset using the test set.\n\n#### 2. Root Mean Squared Error (RMSE):\nRMSE is the square root of the MSE, and it provides a measure of the average difference between the predicted values and the actual values in the original units. The formula for RMSE is:\n\n$$RMSE = \\sqrt{MSE}$$\n\nIn the code, `mean_squared_error(y_test, y_pred)` calculates the RMSE for the dataset.\n\n#### 3. Mean Absolute Error (MAE):\nMAE measures the average absolute difference between the predicted values and the actual values. It is calculated as:\n\n$$MAE = \\frac{1}{n}\\sum_{i}^{n} |y_{i} - \\hat{y}_{i}|$$\n\nIn the code, `mean_absolute_error(y_test, y_pred)` calculates the MAE for the dataset.\n\n#### 4. R-Squared $R^{2}$:\nR-Squared, also known as the coefficient of determination, measures the proportion of the total variation in the target variable that can be explained by the linear regression model. It ranges from 0 to 1, with a higher value indicating a better fit. The formula for R-Squared is:\n\n$$R^{2} = 1 - \\frac{SSR}{SST}$$\n\nwhere:\n\n- $R^{2}$ represents the R-Squared score.\n- $SSR$ is the sum of squared residuals (the sum of squared differences between the predicted values and the actual values).\n- $SST$ is the total sum of squares (the sum of squared differences between the actual values and their mean).\nIn the code, `r2_score(y_test, y_pred)` calculates the $R^{2}$ score for the dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate random data\nX = np.random.rand(200, 1)  # Input feature\ny = 2 + 3 * X + np.random.randn(200, 1)  # True underlying relationship + random noise\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create linear regression model\nmodel = LinearRegression()\n\n# Fit the model on the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error:', mse)\nprint(f'Root Mean Squared Error: {rmse:.2f}')\nprint('Mean Absolute Error:', mae)\nprint('R-squared:', r2)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Mean Squared Error: 1.0846238625454692\nRoot Mean Squared Error: 1.08\nMean Absolute Error: 0.8387402733729704\nR-squared: 0.240964423634343\n"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Unsupervised Learning\n## Dimensionality Reduction Using Principal Component Analysis (PCA)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "PCA (Principal Component Analysis) is a dimensionality reduction technique. It is commonly used to reduce the number of features or variables in a dataset while retaining most of the information present in the original data. Performing PCA can sometimes lower accuracy because it involves reducing the dimensionality of the data by projecting it onto a lower-dimensional subspace. This reduction in dimensionality can result in some loss of information, which may affect the performance of certain algorithms that rely heavily on the original feature space.\n\nDespite potentially lowering accuracy, PCA is still widely used for the following reasons:\n\n1. Dimensionality reduction: Simplifies high-dimensional data by reducing the number of features, making analysis more manageable.\n2. Feature interpretation: Transforms features into uncorrelated principal components, revealing key contributors to variance and underlying data structure.\n3. Noise reduction: Filters out noisy or irrelevant features, enhancing model robustness by focusing on informative aspects.\n4. Computational efficiency: Reduces memory, storage, and computation requirements, making complex algorithms more efficient and scalable, especially for large datasets.\n5. Visualization: Enables effective visualization of high-dimensional data in lower-dimensional spaces, facilitating exploration and communication of insights.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Step 1: Load the iris dataset from plotly.express\ndf = px.data.iris()\n\n# Extract features and target variable\nX = df.drop('species', axis=1).values\ny = df['species'].values\n\n\n# Logistic regression on the original dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(\"Original shape: \", X_train.shape)\nlr_original = LogisticRegression()\nlr_original.fit(X_train, y_train)\ny_pred_original = lr_original.predict(X_test)\ncm = confusion_matrix(y_test, y_pred_original)\ntp = cm[1, 1]  # True Positives\nfp = cm[0, 1]  # False Positives\nfn = cm[1, 0]  # False Negatives\ntn = cm[0, 0]  # True Negatives\naccuracy_original = (tp + tn) / (tp + tn + fp + fn)\nprint('Accuracy on original dataset:', accuracy_original)\n\n# Perform PCA\ncovar_matrix = PCA(n_components=2) # The n_components parameter determines the number of dimensions the data will be reduced to.\nX_train_transformed = covar_matrix.fit_transform(X_train) # fits the PCA model to the training data X_train and simultaneously applies the transformation. \nprint(\"Transformed shape: \", X_train_transformed.shape)\nX_test_transformed = covar_matrix.transform(X_test) # projects the testing data onto the same reduced feature space as the training data\nvariance = covar_matrix.explained_variance_ratio_  # Calculate variance ratios\nvar = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=6) * 100)  # Cumulative sum of variance explained with [n] features\n\n# Plot explained variance\nplt.ylabel('Cumulative Explained Variance')\nplt.xlabel('Number of Components')\nplt.title('PCA Analysis')\nplt.plot(var)\nplt.show()\n\n# Logistic regression on the transformed dataset\nlr_transformed = LogisticRegression()\nlr_transformed.fit(X_train_transformed, y_train)\ny_pred_transformed = lr_transformed.predict(X_test_transformed)\ncm_transformed = confusion_matrix(y_test, y_pred_transformed)\ntp = cm_transformed[1, 1]  # True Positives\nfp = cm_transformed[0, 1]  # False Positives\nfn = cm_transformed[1, 0]  # False Negatives\ntn = cm_transformed[0, 0]  # True Negatives\naccuracy_transformed = (tp + tn) / (tp + tn + fp + fn)\nprint('Accuracy on transformed dataset:', accuracy_transformed)",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<p style='color: #FF0000'>we used the fit_transform() method of PCA on the training set only instead of directly applying transform() to both the training and testing sets in the above code. This is because the fit_transform() method of PCA is used on the training set to learn the transformation based on its statistical properties. Applying transform() directly to the testing set would not utilize the information learned from the training set and could lead to inconsistent transformations, making it difficult to compare and interpret the results.</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Exercises\n### Exercise 1 - Build a linear regression with the dataset below and evaluate the model\nInstruction:\n1. Split the dataset into a training set (70% of the data) and a test set (30% of the data).\n2. Implement linear regression\n3. Train the linear regression model on the training set\n4. Evaluate the trained model's performance on the test set by calculating the Mean Squared Error, Root Mean Squared Error, Mean Absolute Error, R-squared\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Load the california_housing dataset having a continuous target variable.\ndata = datasets.fetch_california_housing()\n# Make a DataFrame\ndf = pd.DataFrame(np.column_stack([data['data'], data['target']]), columns=data['feature_names'] + ['target'])\ndf.head()",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X = df.iloc[:,:-1].values\ny = df['target'].values\n\n#1- Dataset Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(\"Original shape: \", X_train.shape)\n\n#2- Create linear regression model\nmodel = LinearRegression()\n\n#3-  train Model\nmodel.fit(X_train, y_train)\n\n#4- Make predictions on the test dataset.\ny_pred = model.predict(X_test)\n\n#5- Evaluate model performance\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error:', mse)\nprint(f'Root Mean Squared Error: {rmse:.2f}')\nprint('Mean Absolute Error:', mae)\nprint('R-squared:', r2)\n\n\n",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n    <summary>Click here for Solution</summary>\n\n```python\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :8], df['target'], test_size=0.3, random_state=0)\n\n# Create linear regression model\nmodel = LinearRegression()\n\n# Fit the model on the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint('Mean Squared Error:', mse)\nprint(f'Root Mean Squared Error: {rmse:.2f}')\nprint('Mean Absolute Error:', mae)\nprint('R-squared:', r2)\n```\n\n</details>\n",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise 2 - Build a logistic regression with the dataset below and evaluate the model\nInstruction:\n1. Split the dataset into a training set (70% of the data) and a test set (30% of the data).\n2. Implement logistic regression\n3. Train the logistic regression model on the training set\n4. Evaluate the trained model's performance on the test set by calculating the accuracy, precision, recall, and F1 score.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Load the california_housing dataset having a continuous target variable.\ndata = datasets.fetch_california_housing()\n# Make a DataFrame\ndf = pd.DataFrame(np.column_stack([data['data'], data['target']]), columns=data['feature_names'] + ['target'])\ndf['target'] = df['target'].apply(lambda x: 1 if x > df['target'].mean() else 0)\ndf.head()",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X = df.iloc[:,:-1].values\ny = df['target'].values\n\n#1- Dataset Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(\"Original shape: \", X_train.shape)\n\n# Create logistic regression model\nmodel = LogisticRegression()\n\n# Fit the model on the training data\nmodel.fit(X_train, y_train)\n\n#4- Make predictions on the test dataset.\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\n\ncm = confusion_matrix(y_test, y_pred)\ntp = cm[1, 1]  # True Positives\nfp = cm[0, 1]  # False Positives\nfn = cm[1, 0]  # False Negatives\ntn = cm[0, 0]  # True Negatives\n\n# Accuracy\naccuracy = (tp + tn) / (tp + tn + fp + fn)\n# Precision\nprecision = tp / (tp + fp)\n# Recall\nrecall = tp / (tp + fn)\n# Specificity\nspecificity = tn / (tn + fp)\n# F1 Score\nf1 = 2 * (precision * recall) / (precision + recall)\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('Specificity:', specificity)\nprint('F1 score:', f1)\n",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n    <summary>Click here for Solution</summary>\n\n```python\n# Split the data into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :8], df['target'], test_size=0.3, random_state=0)\n\n# Create logistic regression model\nmodel = LogisticRegression()\n\n# Fit the model on the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = model.predict(X_test)\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# True Positives\ntp = cm[1, 1]\n# False Positives\nfp = cm[0, 1]\n# False Negatives\nfn = cm[1, 0]\n# True Negatives\ntn = cm[0, 0]\n\n# Accuracy\naccuracy = (tp + tn) / (tp + tn + fp + fn)\n# Precision\nprecision = tp / (tp + fp)\n# Recall\nrecall = tp / (tp + fn)\n# Specificity\nspecificity = tn / (tn + fp)\n# F1 Score\nf1 = 2 * (precision * recall) / (precision + recall)\n\n\n# Print the evaluation metrics\nprint(\"Confusion Matrix:\", cm)\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint(\"Specificity:\", specificity)\nprint('F1 Score:', f1)\n```\n\n</details>\n",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise 3 - Perform PCA on the data below to plot the corresponding Cumulative Explained Variance\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.random.seed(0)\nX_train = np.random.rand(100, 8)  # 100 samples with 8 features\nX_test = np.random.rand(50, 8)  # 50 samples with 8 features",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Perform PCA\ncovar_matrix = PCA(n_components=8) # The n_components parameter determines the number of dimensions the data will be reduced to.\nX_train_transformed = covar_matrix.fit_transform(X_train) # fits the PCA model to the training data X_train and simultaneously applies the transformation. \nprint(\"Transformed shape: \", X_train_transformed.shape)\nX_test_transformed = covar_matrix.transform(X_test) # projects the testing data onto the same reduced feature space as the training data\nvariance = covar_matrix.explained_variance_ratio_  # Calculate variance ratios\nvar = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=6) * 100)  # Cumulative sum of variance explained with [n] features\n\n# Plot explained variance\nplt.ylabel('Cumulative Explained Variance')\nplt.xlabel('Number of Components')\nplt.title('PCA Analysis')\nplt.plot(var)\nplt.show()",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details>\n    <summary>Click here for Solution</summary>\n\n```python\n# Perform PCA\ncovar_matrix = PCA(n_components=8)  # We have 8 features\nX_train_pca = covar_matrix.fit_transform(X_train)\nX_test_pca = covar_matrix.transform(X_test)\nvariance = covar_matrix.explained_variance_ratio_  # Calculate variance ratios\nvar = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=6) * 100)  # Cumulative sum of variance explained with [n] features\n\n# Plot explained variance\nplt.ylabel('Cumulative Explained Variance')\nplt.xlabel('Number of Components')\nplt.title('PCA Analysis')\nplt.plot(var)\nplt.show()\n```\n\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Congratulations! You have completed the lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n[Vicky Kuo](https://author.skills.network/instructors/vicky_kuo?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkML0104ENSkillsNetwork3497-2023-01-01)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Change Log\n|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n|-|-|-|-|\n|2023-06-19|0.1|Vicky Kuo|Designd and Created the Lab|\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Copyright © 2023 IBM Corporation. All rights reserved.\n",
      "metadata": {}
    }
  ]
}